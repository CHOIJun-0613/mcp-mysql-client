   1. GPU 가속 활용 (가장 중요):
       * Ollama GPU 지원 확인: Ollama는 GPU를 사용하여 모델 추론 속도를 크게 높일 수 있습니다. Docker Desktop 환경에서 GPU를
         사용하려면, Docker Desktop 설정에서 GPU 리소스 할당이 활성화되어 있는지 확인해야 합니다.
       * NVIDIA GPU: NVIDIA GPU를 사용한다면, Docker Desktop이 WSL2(Windows Subsystem for Linux 2)를 통해 CUDA(Compute Unified
         Device Architecture)를 지원하도록 설정되어 있는지 확인하세요. Ollama 컨테이너를 실행할 때 --gpus all 옵션을 사용했는지도
         중요합니다.
          예시: docker run -d --gpus all -v ollama:/root/.ollama -p 11434:11434 --name ollama-container ollama/ollama
       * AMD/Intel GPU: Ollama는 AMD 및 Intel GPU에 대한 실험적인 지원도 제공합니다. 해당 GPU를 사용하신다면 Ollama 공식 문서를
         참조하여 설정 방법을 확인해보세요.

   2. Docker Desktop 리소스 할당 최적화:
       * CPU 및 메모리 할당: Docker Desktop 설정(Settings -> Resources)에서 Ollama 컨테이너에 충분한 CPU 코어와 메모리(RAM)가
         할당되었는지 확인하세요. 기본 설정보다 더 많은 리소스를 할당하면 성능이 향상될 수 있습니다.
       * WSL2 설정 (Windows 사용자): Windows에서 Docker Desktop을 사용한다면, WSL2 백엔드를 사용합니다. WSL2에 할당된 메모리 및
         CPU 리소스를 .wslconfig 파일을 통해 조정할 수 있습니다. %UserProfile%\.wslconfig 파일에 다음 내용을 추가하여 WSL2의
         리소스를 늘릴 수 있습니다 (예시):

   1         [wsl2]
   2         memory=8GB  # 시스템 메모리에 따라 조절
   3         processors=4 # 시스템 CPU 코어 수에 따라 조절
          변경 후에는 wsl --shutdown 명령으로 WSL2를 재시작해야 합니다.

   3. Ollama 모델 최적화:
       * 모델 양자화 (Quantization): gemma:7b 모델은 이미 비교적 작지만, Ollama는 다양한 양자화(예: gemma:7b-instruct-q4_K_M)
         버전을 제공합니다. 더 낮은 비트의 양자화된 모델은 속도가 더 빠를 수 있지만, 정확도가 약간 저하될 수 있습니다. ollama run
         gemma:7b --verbose 명령으로 모델 로딩 시 GPU 사용 여부를 확인할 수 있습니다.
       * 더 작은 모델 사용: gemma:2b와 같이 더 작은 모델을 사용하면 속도가 더 빨라질 수 있습니다. 하지만 이는 LLM의 성능(정확도)과
         트레이드오프 관계에 있습니다.

   4. 네트워크 지연 감소:
       * 로컬 네트워크: FastMCP 서버와 Ollama 컨테이너가 동일한 Docker 네트워크 내에서 통신하도록 구성되어 있는지 확인하세요.
         localhost를 사용하고 있다면 일반적으로 문제가 없지만, 복잡한 네트워크 구성에서는 지연이 발생할 수 있습니다.

   5. FastAPI 애플리케이션 최적화 (부차적):
       * Uvicorn 워커 수: uvicorn을 사용하여 FastAPI 애플리케이션을 실행할 때, --workers 옵션을 사용하여 여러 워커 프로세스를
         실행하면 동시 요청 처리 능력이 향상될 수 있습니다. 하지만 단일 요청의 응답 속도 자체를 크게 개선하지는 않습니다.

  가장 먼저 GPU 가속이 제대로 활성화되어 있는지 확인하시고, 그 다음 Docker Desktop 및 WSL2의 리소스 할당을 최적화하는 것을
  권장합니다. 이 두 가지가 LLM 추론 속도에 가장 큰 영향을 미칠 것입니다.